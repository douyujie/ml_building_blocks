{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1:\n",
    "\n",
    "If $f = \\textbf{a}^TX\\textbf{b}$, solve for: $\\frac{\\partial f}{\\partial X}$, where $\\textbf{a}: (m, 1), X: (m, n), \\textbf{b}: (n, 1)$, $f$ is a scalar\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$df = d\\textbf{a}X\\textbf{b} + \\textbf{a}^TdX\\textbf{b} + \\textbf{a}^TXd\\textbf{b}$, where $\\textbf{a}, \\textbf{b}$ are constants, so $d\\textbf{a} = d\\textbf{b} = 0$\n",
    "\n",
    "and $df$ is a scalar so:\n",
    "\n",
    " $df = tr(df) =  tr(\\textbf{a}^TdX\\textbf{b}) = tr(\\textbf{b}\\textbf{a}^TdX) = tr((\\textbf{a}\\textbf{b}^T)^TdX)$\n",
    "\n",
    "Here we used:\n",
    "\n",
    "$tr(AB) = tr(BA)$ and exchanged $\\textbf{a}^TdX$ and $\\textbf{b}$\n",
    "\n",
    "According to: $df = tr({\\frac{\\partial f}{\\partial X}}^T dX)$ we get:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial X} = \\textbf{a}\\textbf{b}^T$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example 2:\n",
    "\n",
    "If $f = \\textbf{a}^T\\exp(X\\textbf{b})$, solve for $\\frac{\\partial f}{\\partial X}$, where $\\textbf{a}: (m, 1), X: (m, n), \\textbf{b}: (n, 1), \\exp$ is element-wise exp, $f$ is a scalar.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$df = \\textbf{a}^T(\\exp(X\\textbf{b})\\odot(dX\\textbf{b}))$\n",
    "\n",
    "Here we used: \n",
    "\n",
    "$d\\sigma(X) = \\sigma'(X)\\odot dX$, where $\\sigma$ is element-wise function\n",
    "\n",
    "Then: $df = tr(\\textbf{a}^T(\\exp(X\\textbf{b})\\odot(dX\\textbf{b}))) = tr((\\textbf{a}\\odot\\exp(X\\textbf{b}))^TdX\\textbf{b}) = tr(\\textbf{b}(\\textbf{a}\\odot\\exp(X\\textbf{b}))^TdX) = tr(((\\textbf{a}\\odot\\exp(X\\textbf{b}))b^T)^TdX)$\n",
    "\n",
    "where we use: \n",
    "\n",
    "$tr(A^T(B\\odot C)) = tr((A\\odot B)^TC)$ ($A, B, C$ have the same shape) as well as $tr(AB) = tr(BA)$\n",
    "\n",
    "According to $df = tr({\\frac{\\partial f}{\\partial X}}^TdX)$ we get:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial X} = (a\\odot\\exp(x\\textbf{b}))\\textbf{b}^T$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example 3:\n",
    "\n",
    "If $f = tr(Y^TMY), Y = \\sigma(WX)$, solve for: $\\frac{\\partial f}{\\partial X}$, where: $W: (l, m), X: (m, n), Y: (l, n), M: (l, l)$ is symmetric matrix, $\\sigma$ is element-wise function, $f$ is a scalar.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$df = tr((dY)^TMY) + tr(Y^TMdY) = tr(Y^TM^TdY) + tr(Y^TMdY) = tr(Y^T(M+M^T)dY)$\n",
    "\n",
    "Here we used: \n",
    "\n",
    "$d(tr) = tr(dX), tr(A+B) = tr(A)+tr(B), tr(A^T) = tr(A)$\n",
    "\n",
    "According to $df = tr({\\frac{\\partial f}{\\partial Y}}^TdY)$ we get:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial Y} = (M+M^T)^TY = 2MY$, where $M$ is a symmetric matrix.\n",
    "\n",
    "$df = tr({\\frac{\\partial f}{\\partial Y}}^TdY) = tr({\\frac{\\partial f}{\\partial Y}}^T(\\sigma'(WX)\\odot(WdX))) = tr((\\frac{\\partial f}{\\partial Y}\\odot\\sigma'(WX))^TWdX))$\n",
    "\n",
    "Here we used:\n",
    "\n",
    "$d\\sigma(X) = \\sigma'(X) \\odot dX, tr(A^T(B\\odot C) = (A\\odot B)^TC)$, where $A, B, C$ have the same shape.\n",
    "\n",
    "According to $\\partial f= tr({\\frac{\\partial f}{\\partial X}}^TdX)$ we get:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial X} = W^T(\\frac{\\partial f}{\\partial Y}\\odot\\sigma'(WX)) = W^T((2M\\sigma(WX))\\odot\\sigma'(WX))$\n",
    "\n",
    "\n",
    "\n",
    "# Example 4:  Least Squares Estimation\n",
    "\n",
    "$l = \\parallel X\\textbf{w} - \\textbf{y}\\parallel^2$, solve for l's least squares estimation, when $\\frac{\\partial l}{\\partial\\textbf{w}} = 0$. Where $y: (m, 1), X:(m, n) \\textbf{w}: (n, 1), l$ is a scalar.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$l = (X\\textbf{w} - \\textbf{y})^T(X\\textbf{w}-\\textbf{y})$\n",
    "\n",
    "$dl = (Xd\\textbf{w})^T(X\\textbf{w}-\\textbf{y}) + (X\\textbf{w}-\\textbf{y})^T(Xd\\textbf{w}) = 2(X\\textbf{w}-\\textbf{y})^TXd\\textbf{w}$\n",
    "\n",
    "According to $dl = {\\frac{\\partial l}{\\partial\\textbf{w}}}^Td\\textbf{w}$, we get:\n",
    "\n",
    "$\\frac{dl}{d\\textbf{w}} = 2X^T(X\\textbf{w}-\\textbf{y})$\n",
    "\n",
    "When $\\frac{dl}{d\\textbf{w}} = 0$ we have: $X^TX\\textbf{w} = X^T\\textbf{y}$\n",
    "\n",
    "So: $\\textbf{w} = (X^TX)^{-1}X^T\\textbf{y}$\n",
    "\n",
    "\n",
    "\n",
    "# Example 5: Maximum Likelihood Estimation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example 6: Multivariable Logistic Regression\n",
    "\n",
    "$l = -\\textbf{y}^T\\log softmax(W\\textbf{x})$, solve for $\\frac{\\partial l}{\\partial W}$. Where $\\textbf{y}: (m, 1)$ is one-hot label, $W: (m, n)$, $\\textbf{x}: (n, 1)$, $l$ is a scalar. $softmax(a) = \\frac{\\exp(\\textbf{a})}{\\textbf{1}^T\\exp(\\textbf{a})}$, where $\\exp(\\textbf{a})$ is element-wise exp, $\\textbf{1}$ is full-one vector.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Let $a = W\\textbf{x}$, then $l = -\\textbf{y}^T\\log softmax(\\textbf{a})$,\n",
    "\n",
    "It's not hard to get: $\\frac{\\partial l}{\\partial \\textbf{a}} = softmax(\\textbf{a}) - \\textbf{y}$\n",
    "\n",
    "We have: $dl = tr({\\frac{\\partial l}{\\partial\\textbf{a}}}^Td\\textbf{a}) = tr({\\frac{\\partial l}{\\partial\\textbf{b}}}^T W\\textbf{x}) = tr(\\textbf{x}{\\frac{\\partial l}{\\partial\\textbf{a}}}^TdW)$\n",
    "\n",
    "According to: $df = {\\frac{\\partial l}{\\partial W}}^TdW$ we have:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial W} = \\frac{\\partial l}{\\partial\\textbf{a}}\\textbf{x}^T =(softmax(\\textbf{a})-\\textbf{y})\\textbf{X}^T$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example 7:  Two Layer Neaural Network\n",
    "\n",
    "$l = -\\textbf{y}^T\\log softmax(W_2\\sigma(W_1\\textbf{x}))$, solve for $\\frac{\\partial l}{\\partial W_1}$ and $\\frac{\\partial l}{\\partial W_2}$. Where $\\textbf{y}: (m, 1)$ is one-hot lebel, $W_2: (m, p), W_1:(p, n), \\textbf{x}: (n, 1)$, $l$ is a scalar, $softmax(\\textbf{a}) =  \\frac{\\exp(\\textbf{a})}{\\textbf{1}^T\\exp(\\textbf{a})}$, $\\sigma$ is element-wise sigmoid function: $\\sigma(\\textbf{a}) = \\frac{1}{1+\\exp(-\\textbf{a})}$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Let $\\textbf{a}_1 = W_1\\textbf{x}, \\textbf{h}_1 = \\sigma(\\textbf{a}_1), \\textbf{a}_2 = W_2\\textbf{h}_1$\n",
    "\n",
    "Then: $l = -\\textbf{y}^T\\log softmax(\\textbf{a}_2)$\n",
    "\n",
    "It's not hard to get: $\\frac{\\partial l}{\\partial\\textbf{a}_2} = softmax(\\textbf{a}_2) - \\textbf{y}$\n",
    "\n",
    "$dl = tr(\\frac{\\partial l}{\\partial\\textbf{a}_2}d\\textbf{a}_2) = tr({\\frac{\\partial l}{\\partial\\textbf{a}_2}}^TdW_2\\textbf{h}_1) + tr({\\frac{\\partial l}{\\partial\\textbf{a}_2}}^TW_2d\\textbf{h}_1) = tr(\\textbf{h}_1{\\frac{\\partial l}{\\partial\\textbf{a}_2}}^TdW_2) + tr({\\frac{\\partial l}{\\partial\\textbf{a}_2}}^TW_2d\\textbf{h}_1)$\n",
    "\n",
    "Then we get: \n",
    "\n",
    "$\\frac{dl}{dW_2} = \\frac{\\partial l}{\\partial\\textbf{a}_2}\\textbf{h}_1^T = (softmax(\\textbf{a}_2)-\\textbf{y})\\textbf{h}_1^T$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial\\textbf{h}_1} =  W_2^T\\frac{\\partial l}{\\partial\\textbf{a}_2} = W_2^T(softmax(\\textbf{a}_2) - \\textbf{y})$\n",
    "\n",
    "We have:\n",
    "\n",
    "$dl = tr(\\frac{\\partial l}{\\partial\\textbf{h}_1}^Td\\textbf{h}_1) = tr(\\frac{\\partial l}{\\partial\\textbf{h}_1}^T(\\sigma'(\\textbf{a}_1)\\odot d\\textbf{a}_1)) = tr((\\frac{\\partial l}{\\partial\\textbf{h}_1}\\odot\\sigma'(\\textbf{a}_1))^Td\\textbf{a}_1)$\n",
    "\n",
    "Then we get:\n",
    "\n",
    "$\\frac{dl}{d\\textbf{a}_1} = \\frac{\\partial l}{\\partial\\textbf{h}_1}\\odot\\sigma'(\\textbf{a}_1) = W_2^T(softmax(\\textbf{a}_2) - \\textbf{y})\\odot\\sigma'(\\textbf{a}_1)$\n",
    "\n",
    "We still have:\n",
    "\n",
    "$dl = tr({\\frac{\\partial l}{\\partial\\textbf{a}_1}}^Td\\textbf{a}_1) = tr({\\frac{dl}{d\\textbf{a}_1}^T}dW_1\\textbf{x}) = tr(\\textbf{x}{\\frac{\\partial l}{\\partial\\textbf{a}_1}^T}dW_1)$\n",
    "\n",
    "Then we finally get:\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W_1} = \\frac{\\partial l}{\\partial\\textbf{a}_1}\\textbf{x}^T = W_2^T(softmax(\\textbf{a}_2) - \\textbf{y})\\odot\\sigma'(\\textbf{a}_1)\\textbf{x}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
